const OpenAI = require('openai');
const WebSocket = require('ws');
const { Readable } = require('stream');
const { getProviderForModel } = require('../factory.js');

class OpenAIProvider {
    static async validateApiKey(key) {
        if (!key || typeof key !== 'string' || !key.startsWith('sk-')) {
            return { success: false, error: 'Invalid OpenAI API key format.' };
        }

        try {
            const response = await fetch('https://api.openai.com/v1/models', {
                headers: { Authorization: `Bearer ${key}` },
            });

            if (response.ok) {
                return { success: true };
            } else {
                const errorData = await response.json().catch(() => ({}));
                const message = errorData.error?.message || `Validation failed with status: ${response.status}`;
                return { success: false, error: message };
            }
        } catch (error) {
            console.error(`[OpenAIProvider] Network error during key validation:`, error);
            return { success: false, error: 'A network error occurred during validation.' };
        }
    }
}

/**
 * Creates an OpenAI STT session
 * @param {object} opts - Configuration options
 * @param {string} opts.apiKey - OpenAI API key
 * @param {string} [opts.language='en'] - Language code
 * @param {object} [opts.callbacks] - Event callbacks
 * @returns {Promise<object>} STT session
 */
async function createSTT({ apiKey, language = 'en', callbacks = {}, ...config }) {
    const wsUrl = 'wss://api.openai.com/v1/realtime?intent=transcription';

    const headers = {
        Authorization: `Bearer ${apiKey}`,
        'OpenAI-Beta': 'realtime=v1',
    };

    const ws = new WebSocket(wsUrl, { headers });

    return new Promise((resolve, reject) => {
        ws.onopen = () => {
            console.log('WebSocket session opened.');

            const sessionConfig = {
                type: 'transcription_session.update',
                session: {
                    input_audio_format: 'pcm16',
                    input_audio_transcription: {
                        model: 'gpt-4o-mini-transcribe',
                        prompt: config.prompt || '',
                        language: language || 'en',
                    },
                    turn_detection: {
                        type: 'server_vad',
                        threshold: 0.5,
                        prefix_padding_ms: 200,
                        silence_duration_ms: 100,
                    },
                    input_audio_noise_reduction: {
                        type: 'near_field',
                    },
                },
            };

            ws.send(JSON.stringify(sessionConfig));

            // Helper to periodically keep the websocket alive
            const keepAlive = () => {
                try {
                    if (ws.readyState === WebSocket.OPEN) {
                        // The ws library supports native ping frames which are ideal for heart-beats
                        ws.ping();
                    }
                } catch (err) {
                    console.error('[OpenAI STT] keepAlive error:', err.message);
                }
            };

            resolve({
                sendRealtimeInput: audioData => {
                    if (ws.readyState === WebSocket.OPEN) {
                        const message = {
                            type: 'input_audio_buffer.append',
                            audio: audioData,
                        };
                        ws.send(JSON.stringify(message));
                    }
                },
                // Expose keepAlive so higher-level services can schedule heart-beats
                keepAlive,
                close: () => {
                    if (ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify({ type: 'session.close' }));
                        ws.onmessage = ws.onerror = () => {}; // Remove handlers
                        ws.close(1000, 'Client initiated close.');
                    }
                },
            });
        };

        ws.onmessage = event => {
            // ── Filter termination/heartbeat packets ──────────────────────────────
            if (!event.data || event.data === 'null' || event.data === '[DONE]') return;

            let msg;
            try {
                msg = JSON.parse(event.data);
            } catch {
                return;
            } // Ignore JSON parsing failures

            if (!msg || typeof msg !== 'object') return;

            msg.provider = 'openai'; // ← Always specify
            callbacks.onmessage?.(msg);
        };

        ws.onerror = error => {
            console.error('WebSocket error:', error.message);
            if (callbacks && callbacks.onerror) {
                callbacks.onerror(error);
            }
            reject(error);
        };

        ws.onclose = event => {
            console.log(`WebSocket closed: ${event.code} ${event.reason}`);
            if (callbacks && callbacks.onclose) {
                callbacks.onclose(event);
            }
        };
    });
}

/**
 * Creates an OpenAI LLM instance
 * @param {object} opts - Configuration options
 * @param {string} opts.apiKey - OpenAI API key
 * @param {string} [opts.model='gpt-4.1'] - Model name
 * @param {number} [opts.temperature=0.7] - Temperature
 * @param {number} [opts.maxTokens=2048] - Max tokens
 * @returns {object} LLM instance
 */
function createLLM({ apiKey, model = 'gpt-4.1', temperature = 0.7, maxTokens = 2048, ...config }) {
    const client = new OpenAI({ apiKey });

    const callApi = async messages => {
        const response = await client.chat.completions.create({
            model: model,
            messages: messages,
            temperature: temperature,
            max_tokens: maxTokens,
        });
        return {
            content: response.choices[0].message.content.trim(),
            raw: response,
        };
    };

    return {
        generateContent: async parts => {
            const messages = [];
            let systemPrompt = '';
            let userContent = [];

            for (const part of parts) {
                if (typeof part === 'string') {
                    if (systemPrompt === '' && part.includes('You are')) {
                        systemPrompt = part;
                    } else {
                        userContent.push({ type: 'text', text: part });
                    }
                } else if (part.inlineData) {
                    userContent.push({
                        type: 'image_url',
                        image_url: { url: `data:${part.inlineData.mimeType};base64,${part.inlineData.data}` },
                    });
                }
            }

            if (systemPrompt) messages.push({ role: 'system', content: systemPrompt });
            if (userContent.length > 0) messages.push({ role: 'user', content: userContent });

            const result = await callApi(messages);

            return {
                response: {
                    text: () => result.content,
                },
                raw: result.raw,
            };
        },

        // For compatibility with chat-style interfaces
        chat: async messages => {
            return await callApi(messages);
        },
    };
}

/**
 * Creates an OpenAI streaming LLM instance
 * @param {object} opts - Configuration options
 * @param {string} opts.apiKey - OpenAI API key
 * @param {string} [opts.model='gpt-4.1'] - Model name
 * @param {number} [opts.temperature=0.7] - Temperature
 * @param {number} [opts.maxTokens=2048] - Max tokens
 * @returns {object} Streaming LLM instance
 */
function createStreamingLLM({ apiKey, model = 'gpt-4.1', temperature = 0.7, maxTokens = 2048, ...config }) {
    return {
        streamChat: async messages => {
            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    Authorization: `Bearer ${apiKey}`,
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    model: model,
                    messages,
                    temperature,
                    max_tokens: maxTokens,
                    stream: true,
                }),
            });

            if (!response.ok) {
                throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
            }

            return response;
        },
    };
}

module.exports = {
    OpenAIProvider,
    createSTT,
    createLLM,
    createStreamingLLM,
};
